{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc8275f3",
   "metadata": {},
   "source": [
    "# Predictive Analtics Exam 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132c949a",
   "metadata": {},
   "source": [
    "### 1. True or False\n",
    "\n",
    "a) Random forest is not an ensemble learning technique. FALSE\n",
    "\n",
    "b) Support vector machine is an ensemble learning technique. TRUE\n",
    "\n",
    "c) If the classifier produce a 80% recall on test dataset (using default values for hyperparameters), there is no need to tune the classifier hyper-parameters. FALSE\n",
    "\n",
    "d) Random search and grid search always produce the same results. FALSE\n",
    "\n",
    "e) Increasing the value of max depth in tree-based models prevents overfitting. FALSE\n",
    "\n",
    "f) Scaling the input variables in tree-based models can speed up the hyper-parameter tuning process. FALSE\n",
    "\n",
    "g) Random search is preferred when the training dataset is big and the number of hyperparameter combinations is large. TRUE\n",
    "\n",
    "h) XGBoost always outperforms random forest. FALSE\n",
    "\n",
    "i) XGBoost is faster than regular gradient boosting algorithms because it trains the the base-learners in parallel.\n",
    "\n",
    "j) In order to avoid overfitting, it is recommended to tune the learning rate hyperparameter in random forest models. FALSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e6b92",
   "metadata": {},
   "source": [
    "### 2. How does stacking work? Be specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d04665",
   "metadata": {},
   "source": [
    "### 3.  If you have trained three different models on the exact same training dataset and they all achieve a 93% F1-score on the exact same dataset, is there any chance that you can combine these three models to get better results? If so, how? If not, why? Be specific.\n",
    "\n",
    "Yes, you could still combine these three models to potentially get better results. By extracting the test data-frame likelihoods from each of the considered model types, you can create an ensemble learning model which takes these likelihoods as input variables to predict the class. Even if all thre models have already achieved an F1-Score of 93%, it is still possible for an ensemble model to improve prediction power because it can find new patterns in the estimated likelihoods and produce its own. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4212c1f9",
   "metadata": {},
   "source": [
    "### 4. If your AdaBoost model overfits the data (probably because the number of boosted trees is large), what hyper-parameter you need to tune? Be specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45c9080",
   "metadata": {},
   "source": [
    "### 5. Which of the following algorithm is NOT an ensemble learning algorithm?\n",
    "\n",
    "(a) support vector machine is not an ensemble learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c592a80",
   "metadata": {},
   "source": [
    "### 6. How does XGBoost with random forest as base learner work? Be specific.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7283ffd1",
   "metadata": {},
   "source": [
    "### 7. How do we select the best hyper-parameter combination for a given model?\n",
    "\n",
    "(b) Performance on the validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f287a",
   "metadata": {},
   "source": [
    "### 8. The below chart shows the performance of a random forest models (with different number of trees). Using the above chart, answer parts (a)-(b). \n",
    "\n",
    "### (a) If a data scientist wants to tune the number of trees based on the F1-score, how many trees would he select? Be specific. \n",
    "To tune the number of trees based on the F1-score, the data scientist would select 400 trees because there is little to no prediction improvement in the training and testing sets after increasing the number of trees past 400. Anything more would result in an increased computation time with no model improvement.\n",
    "\n",
    "### (b) What is the reason there is big difference in the F1-score performance between the Train and Test datasets? Be specific.\n",
    "The main reason why there would be a huge difference in the F1-score performance between the train and test datasets is that the model is overfitting the data. Typically with hyper-parameter tuning, we create a model that fits the training data very well, but does not do a great job of generalizing on the test set. This results in a huge performance loss on the testing set. Another reason could be that the test data set is not representative of the training set which would also decrease test performance, but this is much less likely to be the root error compared to than training an over-fit model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ecf826",
   "metadata": {},
   "source": [
    "### 9.  What is the difference between homogeneous and heterogeneous ensembles? Be specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c646c16",
   "metadata": {},
   "source": [
    "### 10 Random forest is an example of\n",
    "(a) Homogeneous ensemble\n",
    "(b) Heterogeneous ensemble\n",
    "(c) Bagging\n",
    "(c) (a) and (b)\n",
    "(d) (a) and (c)\n",
    "(e) (b) and (c)\n",
    "(f) None of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d48c9a",
   "metadata": {},
   "source": [
    "## 11. A junior data scientist is working on building a classification framework that can help a small local credit union to identify fraudulent transactions. The data scientist collects a very small dataset with 100 observations, from which 4 observations are labelled as fraudulent, to start building and tuning the model. As part of the tuning process, the data scientist considers using the GridSearch function from scikit-learn with cv = 5. Explain why this approach is a good idea or not. Be specific.\n",
    "\n",
    "This sounds like a very poor idea. The GridSearchCV framework uses a stratified k-fold approach, which means that by setting cv = 5, the framework splits the complete 100 observation set into five data sets that are meant to contained a stratified sample of the class of interest (fraud transaction in this case). However, since there are not even five positive fraud cases, the GridSearchCV function would be (a) unable to split the data for the purpose of cross validation, or (b) able to split the data but one of the resulting data sets would have no positive fraud cases. Whichever one of these options is the case, it is a bad idea all around to use the GridSearch function in this scenario. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2154d977",
   "metadata": {},
   "source": [
    "### 12. The typical hyper-parameters in a XGBoost model that data science practitioners tune are: n estimators, learning rate, max depth, min child weight, subsample, and colsample bytree.\n",
    "\n",
    "### (a) What is/are the hyper-parameters that may cause overfitting if we increase their values while holding the values of the hyper-parameters constant? Be specific.\n",
    "\n",
    "\n",
    "### (b) What is/are the hyper-parameters that prevents overfitting if we increase their values while holding the values of the hyper-parameters constant? Be specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea77c88",
   "metadata": {},
   "source": [
    "### 13. In tree-based models, how does a data scientist optimize the number of trees? Be specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b17e5bb",
   "metadata": {},
   "source": [
    "### 14. A Machine Learning Specialist is assigned to a team that is responsible for tuning an XGBoost model to ensure that it performs correctly on test data. However, when dealing with unknown data, this does not operate as planned. The following table summarizes the existing hyper-parameters:\n",
    "\n",
    "XGBoost_params = {’n_estimators’: 2000, ’max_depth’: 30, ’min_child_weight’: 3, ’subsample’: 0.9, ’objective’: ’reg:squarederror’}\n",
    "\n",
    "### (a) What type of task the machine learning specialist is working on? Classification or regression? Be specific.\n",
    "\n",
    "\n",
    "### (b) Given the existing XGBoost hyper-parameters, what may be the reason that the XGBoost model is not performing as expected? Be specific.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a537d432",
   "metadata": {},
   "source": [
    "### 15. Considering the train.csv and test.csv data files containing information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. The goal is to predict default payment next month on the test.csv data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e574a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "096b82ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing necessary libraries\n",
    "\n",
    "import boto3\n",
    "import pandas as pd; pd.set_option('display.max_columns', 50)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import precision_recall_cutoff_exam2 as prc\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eae9050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>PAY_6</th>\n",
       "      <th>BILL_AMT1</th>\n",
       "      <th>BILL_AMT2</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>400000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55773</td>\n",
       "      <td>55917</td>\n",
       "      <td>51389</td>\n",
       "      <td>48272</td>\n",
       "      <td>49478</td>\n",
       "      <td>51242</td>\n",
       "      <td>3028</td>\n",
       "      <td>3023</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>38662</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>140</td>\n",
       "      <td>3230</td>\n",
       "      <td>3011</td>\n",
       "      <td>1964</td>\n",
       "      <td>1883</td>\n",
       "      <td>1538</td>\n",
       "      <td>3230</td>\n",
       "      <td>3011</td>\n",
       "      <td>1964</td>\n",
       "      <td>1883</td>\n",
       "      <td>1538</td>\n",
       "      <td>1911</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>270000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59710</td>\n",
       "      <td>49986</td>\n",
       "      <td>104390</td>\n",
       "      <td>94856</td>\n",
       "      <td>86461</td>\n",
       "      <td>83650</td>\n",
       "      <td>1808</td>\n",
       "      <td>69563</td>\n",
       "      <td>2891</td>\n",
       "      <td>2689</td>\n",
       "      <td>3012</td>\n",
       "      <td>2771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>280000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>280913</td>\n",
       "      <td>283222</td>\n",
       "      <td>273160</td>\n",
       "      <td>257689</td>\n",
       "      <td>193231</td>\n",
       "      <td>191143</td>\n",
       "      <td>11052</td>\n",
       "      <td>9563</td>\n",
       "      <td>15017</td>\n",
       "      <td>5374</td>\n",
       "      <td>5420</td>\n",
       "      <td>6021</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>1512</td>\n",
       "      <td>2458</td>\n",
       "      <td>664</td>\n",
       "      <td>1814</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>664</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0     400000    1          1         2   32      0      0      0      0   \n",
       "1     120000    2          2         2   30     -1     -1     -1     -1   \n",
       "2     270000    2          2         2   32      0      0      0      0   \n",
       "3     280000    2          2         1   27      0      0      0      0   \n",
       "4      30000    2          1         2   27      0      0     -1      0   \n",
       "\n",
       "   PAY_5  PAY_6  BILL_AMT1  BILL_AMT2  BILL_AMT3  BILL_AMT4  BILL_AMT5  \\\n",
       "0      0      0      55773      55917      51389      48272      49478   \n",
       "1     -1     -1        140       3230       3011       1964       1883   \n",
       "2      0      0      59710      49986     104390      94856      86461   \n",
       "3      0      0     280913     283222     273160     257689     193231   \n",
       "4      0     -2       1512       2458        664       1814          0   \n",
       "\n",
       "   BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \\\n",
       "0      51242      3028      3023      3000      3000      3000     38662   \n",
       "1       1538      3230      3011      1964      1883      1538      1911   \n",
       "2      83650      1808     69563      2891      2689      3012      2771   \n",
       "3     191143     11052      9563     15017      5374      5420      6021   \n",
       "4          0      1000       664      1500         0         0         0   \n",
       "\n",
       "   default payment next month  \n",
       "0                           0  \n",
       "1                           0  \n",
       "2                           0  \n",
       "3                           0  \n",
       "4                           0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## a) Using the pandas library to read the train.csv and test.csv data files and create two data-frames called train and test\n",
    "\n",
    "## Defining the bucket\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'data-448-bucket-callaghan'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "file_key = 'train(1).csv'\n",
    "file_key2 = 'test(1).csv'\n",
    "\n",
    "bucket_object = bucket.Object(file_key)\n",
    "bucket_object2 = bucket.Object(file_key2)\n",
    "\n",
    "file_object = bucket_object.get()\n",
    "file_object2 = bucket_object2.get()\n",
    "\n",
    "file_content_stream = file_object.get('Body')\n",
    "file_content_stream2 = file_object2.get('Body')\n",
    "\n",
    "train = pd.read_csv(file_content_stream)\n",
    "test = pd.read_csv(file_content_stream2)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8b4d45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Engineering features from Exam 1\n",
    "\n",
    "## Train set:\n",
    "\n",
    "## Most common repayment status\n",
    "train['Most_Common'] = np.nan\n",
    "for i in range(0, train.shape[0]):\n",
    "    train.at[i, 'Most_Common'] = train[['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']].loc[i].mode()[0]\n",
    "\n",
    "## From plot tree:\n",
    "train['Tree2'] = np.where((train['PAY_0'] <= 1.5) & (train['PAY_2'] <= 1.5) & (train['PAY_AMT3'] > 395.0), 1, 0)\n",
    "train['Tree6'] = np.where((train['PAY_0'] > 1.5) & (train['PAY_6'] <= 1.0) & (train['BILL_AMT1'] > 649.5), 1, 0)\n",
    "train['Tree7'] = np.where((train['PAY_0'] > 1.5) & (train['PAY_6'] > 1.0) & (train['PAY_AMT3'] <= 14177.0), 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "## Test set:\n",
    "\n",
    "## Most common repayment status\n",
    "test['Most_Common'] = np.nan\n",
    "for i in range(0, test.shape[0]):\n",
    "    test.at[i, 'Most_Common'] = test[['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']].loc[i].mode()[0]\n",
    "\n",
    "## From plot tree:\n",
    "test['Tree2'] = np.where((test['PAY_0'] <= 1.5) & (test['PAY_2'] <= 1.5) & (test['PAY_AMT3'] > 395.0), 1, 0)\n",
    "test['Tree6'] = np.where((test['PAY_0'] > 1.5) & (test['PAY_6'] <= 1.0) & (test['BILL_AMT1'] > 649.5), 1, 0)\n",
    "test['Tree7'] = np.where((test['PAY_0'] > 1.5) & (test['PAY_6'] > 1.0) & (test['PAY_AMT3'] <= 14177.0), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39e5fe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## b) Splitting the train data-frame intro training (80%) and validation (20%) (taking into account the proportions of 0s and 1s)\n",
    "\n",
    "## Defining the input and target variables\n",
    "X = train.drop(columns = ['default payment next month'])\n",
    "Y = train['default payment next month']\n",
    "\n",
    "## Splitting the data\n",
    "X_training, X_validation, Y_training, Y_validation = train_test_split(X, Y, test_size = 0.2, stratify = Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd45116e",
   "metadata": {},
   "source": [
    "#### Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c45bdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Cutoff of Random Forest Model: 0.29\n",
      "\n",
      "F1-Score of Random Forest Model: 52.28 %\n"
     ]
    }
   ],
   "source": [
    "## c) Using the top 7 variables from Exercise 15 part (e) in Exam 1 to  build a model on the training data-frame\n",
    "\n",
    "## Redefining the input and target variables\n",
    "X_training = X_training[['PAY_0', 'PAY_2', 'Most_Common', 'Tree2', 'Tree6', 'PAY_3', 'Tree7']]\n",
    "X_validation = X_validation[['PAY_0', 'PAY_2', 'Most_Common', 'Tree2', 'Tree6', 'PAY_3', 'Tree7']]\n",
    "test = test[['PAY_0', 'PAY_2', 'Most_Common', 'Tree2', 'Tree6', 'PAY_3', 'Tree7']]\n",
    "\n",
    "## Building a Random Forest Model with default hyper-parameters\n",
    "rf_md = RandomForestClassifier().fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the validation set\n",
    "rf_preds = rf_md.predict_proba(X_validation)[:, 1]\n",
    "\n",
    "## Extracting estimated labels using the PRC function\n",
    "rf_labels = prc.precision_recall_cutoff(Y_validation, rf_preds)\n",
    "\n",
    "## Extracting optimal cutoff using the PRC function\n",
    "rf_cutoff = prc.precision_recall_cutoff_cutoff(Y_validation, rf_preds)\n",
    "\n",
    "## Reporting the optimal cutoff value\n",
    "print('Optimal Cutoff of Random Forest Model:', round(rf_cutoff, 3))\n",
    "\n",
    "## Reporting the F1-Score of the model\n",
    "print('\\nF1-Score of Random Forest Model:', round(f1_score(Y_validation, rf_labels) * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed56733a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal hyper-parameters for Random Forest Model: \n",
      " {'max_depth': 7, 'min_samples_leaf': 15, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "\n",
      "Optimal F1-Score:\n",
      " 47.39\n"
     ]
    }
   ],
   "source": [
    "## Tuning the Random Forest model on the validation data-frame\n",
    "\n",
    "## Defining the parameter dictionary\n",
    "rf_param_grid = {'n_estimators': [100, 300, 500], 'max_depth': [3, 5, 7], 'min_samples_split': [5, 10, 15], \n",
    "                  'min_samples_leaf': [5, 10, 15]}\n",
    "\n",
    "## Running GridSearchCV with 3 folds\n",
    "rf_grid_search = GridSearchCV(RandomForestClassifier(), rf_param_grid, cv = 3, scoring = 'f1', n_jobs = -1).fit(X_validation, Y_validation)\n",
    "\n",
    "## Extracting the best hyper-parameters\n",
    "print('Optimal hyper-parameters for Random Forest Model: \\n', rf_grid_search.best_params_)\n",
    "print('\\nOptimal F1-Score:\\n', round(rf_grid_search.best_score_ * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ca418e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Cutoff of Random Forest Model: 0.264\n",
      "\n",
      "F1-Score of Random Forest Model: 52.69 %\n"
     ]
    }
   ],
   "source": [
    "## Building a Random Forest model with the optimal hyper-parameters\n",
    "rf_md = RandomForestClassifier(max_depth = 7, min_samples_leaf = 10, min_samples_split = 10, \n",
    "                               n_estimators = 100).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the validation set\n",
    "rf_preds = rf_md.predict_proba(X_validation)[:, 1]\n",
    "\n",
    "## Extracting estimated labels using the PRC function\n",
    "rf_labels = prc.precision_recall_cutoff(Y_validation, rf_preds)\n",
    "\n",
    "## Extracting optimal cutoff using the PRC function\n",
    "rf_cutoff = prc.precision_recall_cutoff_cutoff(Y_validation, rf_preds)\n",
    "\n",
    "## Reporting the optimal cutoff value\n",
    "print('Optimal Cutoff of Random Forest Model:', round(rf_cutoff, 3))\n",
    "\n",
    "## Reporting the F1-Score of the model\n",
    "print('\\nF1-Score of Random Forest Model:', round(f1_score(Y_validation, rf_labels) * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f957fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally, using the optimal model to predict the likelihood of default payment next month on the test\n",
    "\n",
    "rf_test_preds = rf_md.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8345b6",
   "metadata": {},
   "source": [
    "#### AdaBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de93b6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Cutoff of AdaBoost Model: 0.494\n",
      "\n",
      "F1-Score of AdaBoost Model: 52.19 %\n"
     ]
    }
   ],
   "source": [
    "## d) Using the top 7 variables from Exercise 15 part (e) in Exam 1 to  build a model on the training data-frame\n",
    "\n",
    "## Building an AdaBoost Model with default hyper-parameters\n",
    "ada_md = AdaBoostClassifier().fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the validation set\n",
    "ada_preds = ada_md.predict_proba(X_validation)[:, 1]\n",
    "\n",
    "## Extracting estimated labels using the PRC function\n",
    "ada_labels = prc.precision_recall_cutoff(Y_validation, ada_preds)\n",
    "\n",
    "## Extracting optimal cutoff using the PRC function\n",
    "ada_cutoff = prc.precision_recall_cutoff_cutoff(Y_validation, ada_preds)\n",
    "\n",
    "## Reporting the optimal cutoff value\n",
    "print('Optimal Cutoff of AdaBoost Model:', round(ada_cutoff, 3))\n",
    "\n",
    "## Reporting the F1-Score of the model\n",
    "print('\\nF1-Score of AdaBoost Model:', round(f1_score(Y_validation, ada_labels) * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0947a75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal hyper-parameters for AdaBoost Model: \n",
      " {'base_estimator__max_depth': 7, 'base_estimator__min_samples_leaf': 15, 'base_estimator__min_samples_split': 10, 'learning_rate': 0.1, 'n_estimators': 300}\n",
      "\n",
      "Optimal F1-Score:\n",
      " 0.4560229647315897\n"
     ]
    }
   ],
   "source": [
    "## Tuning the AdaBoost model on the validation data-frame\n",
    "\n",
    "## Defining the parameter dictionary\n",
    "ada_param_grid = {'n_estimators': [100, 300, 500], 'base_estimator__min_samples_split': [10, 15], \n",
    "                  'base_estimator__min_samples_leaf': [10, 15], 'base_estimator__max_depth': [3, 5, 7], \n",
    "                  'learning_rate': [0.001, 0.01, 0.1]}\n",
    "\n",
    "## Running GridSearchCV with 3 folds\n",
    "ada_grid_search = GridSearchCV(AdaBoostClassifier(base_estimator = DecisionTreeClassifier()), ada_param_grid, \n",
    "                               cv = 3, scoring = 'f1', n_jobs = -1).fit(X_validation, Y_validation)\n",
    "\n",
    "## Extracting the best hyper-parameters\n",
    "print('Optimal hyper-parameters for AdaBoost Model: \\n', ada_grid_search.best_params_)\n",
    "print('\\nOptimal F1-Score:\\n', round(ada_grid_search.best_score_ * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a396f8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Cutoff of AdaBoost Model: 0.49\n",
      "\n",
      "F1-Score of AdaBoost Model: 52.61 %\n"
     ]
    }
   ],
   "source": [
    "## Building a AdaBoost model with the optimal hyper-parameters\n",
    "ada_md = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(min_samples_split = 10, min_samples_leaf = 15, max_depth = 7), \n",
    "                            n_estimators = 300, learning_rate = 0.1).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the validation set\n",
    "ada_preds = ada_md.predict_proba(X_validation)[:, 1]\n",
    "\n",
    "## Extracting estimated labels using the PRC function\n",
    "ada_labels = prc.precision_recall_cutoff(Y_validation, ada_preds)\n",
    "\n",
    "## Extracting optimal cutoff using the PRC function\n",
    "ada_cutoff = prc.precision_recall_cutoff_cutoff(Y_validation, ada_preds)\n",
    "\n",
    "## Reporting the optimal cutoff value\n",
    "print('Optimal Cutoff of AdaBoost Model:', round(ada_cutoff, 3))\n",
    "\n",
    "## Reporting the F1-Score of the model\n",
    "print('\\nF1-Score of AdaBoost Model:', round(f1_score(Y_validation, ada_labels) * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bba2e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally, using the optimal model to predict the likelihood of default payment next month on the test\n",
    "\n",
    "ada_test_preds = ada_md.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75d4e76",
   "metadata": {},
   "source": [
    "#### XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6ab734f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Cutoff of XGBoost Model: 0.236\n",
      "\n",
      "F1-Score of XGBoost Model: 52.29 %\n"
     ]
    }
   ],
   "source": [
    "## e) Using the top 7 variables from Exercise 15 part (e) in Exam 1 to  build a model on the training data-frame\n",
    "\n",
    "## Building XGBoost Model with default hyper-parameters\n",
    "xgb_md = XGBClassifier(eval_metric = 'error', use_label_encoder = False).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the validation set\n",
    "xgb_preds = xgb_md.predict_proba(X_validation)[:, 1]\n",
    "\n",
    "## Extracting estimated labels using the PRC function\n",
    "xgb_labels = prc.precision_recall_cutoff(Y_validation, xgb_preds)\n",
    "\n",
    "## Extracting optimal cutoff using the PRC function\n",
    "xgb_cutoff = prc.precision_recall_cutoff_cutoff(Y_validation, xgb_preds)\n",
    "\n",
    "## Reporting the optimal cutoff value\n",
    "print('Optimal Cutoff of XGBoost Model:', round(xgb_cutoff, 3))\n",
    "\n",
    "## Reporting the F1-Score of the model\n",
    "print('\\nF1-Score of XGBoost Model:', round(f1_score(Y_validation, xgb_labels) * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172ee7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tuning the XGBoost model on the validation data-frame\n",
    "\n",
    "## Defining the parameter dictionary\n",
    "xgb_param_grid = {'n_estimators': [300, 500], 'max_depth': [5, 7], 'min_child_weight': [5, 7], \n",
    "                  'learning_rate' : [0.01, 0.001], 'gamma': [0.1, 0.01], 'subsample': [0.8, 1], \n",
    "                  'colsample_bytree': [0.8, 1], 'early_stopping_rounds': [100]}\n",
    "\n",
    "## Running GridSearchCV with 3 folds\n",
    "xgb_grid_search = GridSearchCV(XGBClassifier(eval_metric = 'error', use_label_encoder = False), xgb_param_grid, cv = 3, scoring = 'f1', \n",
    "                               n_jobs = -1).fit(X_validation, Y_validation)\n",
    "\n",
    "## Extracting the best hyper-parameters\n",
    "print('Optimal hyper-parameters for XGBoost Model: \\n', xgb_grid_search.best_params_)\n",
    "print('\\nOptimal F1-Score:\\n', round(xgb_grid_search.best_score_ * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6e5d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building a XGBoost model with the optimal hyper-parameters\n",
    "xgb_md = XGBClassifier(n_estimators = , max_depth = , min_child_weight = , learning_rate = ,\n",
    "                      gamma = , subsample = , colsample_bytree = , early_stopping_rounds = ).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the validation set\n",
    "xgb_preds = xgb_md.predict_proba(X_validation)[:, 1]\n",
    "\n",
    "## Extracting estimated labels using the PRC function\n",
    "xgb_labels = prc.precision_recall_cutoff(Y_validation, xgb_preds)\n",
    "\n",
    "## Extracting optimal cutoff using the PRC function\n",
    "xgb_cutoff = prc.precision_recall_cutoff_cutoff(Y_validation, xgb_preds)\n",
    "\n",
    "## Reporting the optimal cutoff value\n",
    "print('Optimal Cutoff of XGBoost Model:', round(xgb_cutoff, 3))\n",
    "\n",
    "## Reporting the F1-Score of the model\n",
    "print('\\nF1-Score of XGBoost Model:', round(f1_score(Y_validation, xgb_labels) * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa0d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally, using the optimal model to predict the likelihood of default payment next month on the test\n",
    "\n",
    "xgb_test_preds = xgb_md.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a214a4",
   "metadata": {},
   "source": [
    "#### Meta-Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82931c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## f) Using the predictions on the validation data-frame as inputs from parts (c)-(d)-(e) and the actual default payment \n",
    "## next month values from the validation data-frame to build a meta-learner to predict default payment next month\n",
    "\n",
    "## Building the meta-learner data frame\n",
    "meta_data = pd.Dataframe({'rf': rf_preds, 'ada': ada_preds, 'xgb': xgb_preds, 'Y_validation': Y_validation})\n",
    "\n",
    "## Building the meta-learner test data frame\n",
    "meta_data_test = pd.Dataframe({'rf': rf_test_preds, 'ada': ada_test_preds, 'xgb': xgb_test_preds})\n",
    "\n",
    "## Defining the input and target variables\n",
    "X_meta = meta_data[['rf', 'ada', 'xgb']]\n",
    "Y_meta = meta_data['Y_validation']\n",
    "\n",
    "## Defining the parameter dictionary\n",
    "meta_param_grid = {'n_estimators': [100, 300, 500], 'max_depth': [3, 5, 7], 'min_samples_split': [5, 10, 15], \n",
    "                  'min_samples_leaf': [5, 10, 15]}\n",
    "\n",
    "## Running GridSearchCV with 3 folds\n",
    "meta_grid_search = GridSearchCV(RandomForestClassifier(), meta_param_grid, cv = 3, scoring = 'f1', n_jobs = -1).fit(X_meta, Y_meta)\n",
    "\n",
    "## Extracting the best hyper-parameters\n",
    "print('Optimal hyper-parameters for Meta-Learner Random Forest Model: \\n', meta_grid_search.best_params_)\n",
    "print('\\nOptimal F1-Score:\\n', round(meta_grid_search.best_score_ * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5960271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the data\n",
    "X_meta_training, X_meta_validation, Y_meta_training, Y_meta_validation = train_test_split(X_meta, Y_meta, \n",
    "                                                                                          test_size = 0.2, stratify = Y_meta)\n",
    "\n",
    "## Building a Meta-Learner model with the optimal hyper-parameters\n",
    "meta_md = RandomForestClassifier(n_estimators = , max_depth = , min_samples_split = , \n",
    "                                 min_samples_leaf = ).fit(X_meta_training, Y_meta_training)\n",
    "\n",
    "## Predicting on the validation set\n",
    "meta_preds = meta_md.predict_proba(X_meta_validation)[:, 1]\n",
    "\n",
    "## Extracting estimated labels using the PRC function\n",
    "meta_labels = prc.precision_recall_cutoff(Y_meta_validation, meta_preds)\n",
    "\n",
    "## Extracting optimal cutoff using the PRC function\n",
    "meta_cutoff = prc.precision_recall_cutoff_cutoff(Y_meta_validation, meta_preds)\n",
    "\n",
    "## Reporting the optimal cutoff value\n",
    "print('Optimal Cutoff of Meta-Learner Model:', round(meta_cutoff, 3))\n",
    "\n",
    "## Reporting the F1-Score of the model\n",
    "print('\\nF1-Score of Meta-Learner Model:', round(f1_score(Y_meta_validation, meta_labels) * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da302ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally, using the optimal meta-learner to predict the likelihood of default payment next month on the test\n",
    "meta_test_preds = meta_md.predict_proba(meta_data_test)[:, 1]\n",
    "\n",
    "## Creating a data-frame to store results\n",
    "meta_likelihoods = pd.DataFrame({'Likelihoods': meta_test_preds})\n",
    "\n",
    "## Exporting predicted likelihoods as a csv file\n",
    "meta_likelihoods.to_csv('meta_likelihoods.csv', index = False)\n",
    "\n",
    "## Associated cutoff value = 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
