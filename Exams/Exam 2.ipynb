{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed7f1b0",
   "metadata": {},
   "source": [
    "# Predictive Analytics Exam 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99481e4b",
   "metadata": {},
   "source": [
    "### 1. True or False\n",
    "\n",
    "a) Random forest is not an ensemble learning technique. FALSE\n",
    "\n",
    "b) Support vector machine is an ensemble learning technique. FALSE\n",
    "\n",
    "c) If the classifier produce a 80% recall on test dataset (using default values for hyperparameters), there is no need to tune the classifier hyper-parameters. FALSE\n",
    "\n",
    "d) Random search and grid search always produce the same results. FALSE\n",
    "\n",
    "e) Increasing the value of max depth in tree-based models prevents overfitting. FALSE\n",
    "\n",
    "f) Scaling the input variables in tree-based models can speed up the hyper-parameter tuning process. FALSE\n",
    "\n",
    "g) Random search is preferred when the training dataset is big and the number of hyperparameter combinations is large. TRUE\n",
    "\n",
    "h) XGBoost always outperforms random forest. FALSE\n",
    "\n",
    "i) XGBoost is faster than regular gradient boosting algorithms because it trains the the base-learners in parallel. TRUE\n",
    "\n",
    "j) In order to avoid overfitting, it is recommended to tune the learning rate hyperparameter in random forest models. FALSE (not applicable for RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5616ad30",
   "metadata": {},
   "source": [
    "### 2. How does stacking work? Be specific.\n",
    "\n",
    "Stacking is an ensemble machine learning algorithm that involves combining the predictions from multiple machine learning models on the same dataset in order to create a meta-learner. A meta-learner takes the individual model predictions as inputs and the actual responses as outputs to train a higher-level\n",
    "learner. Essentially, stacking uses individual model predictions to create a meta-learner that can provide increased prediction performance, no matter how good the individual models performed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92bb3a7",
   "metadata": {},
   "source": [
    "### 3.  If you have trained three different models on the exact same training dataset and they all achieve a 93% F1-score on the exact same dataset, is there any chance that you can combine these three models to get better results? If so, how? If not, why? Be specific.\n",
    "\n",
    "Yes, you could still combine these three models to potentially get better results. By extracting the test data-frame likelihoods from each of the considered model types, you can create an ensemble learning model which takes these likelihoods as input variables to predict the class. Even if all three models have already achieved an F1-Score of 93%, it is still possible for an ensemble model to improve prediction power because it can find new patterns in the estimated likelihoods and produce its own. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeff837",
   "metadata": {},
   "source": [
    "### 4. If your AdaBoost model overfits the data (probably because the number of boosted trees is large), what hyper-parameter you need to tune? Be specific.\n",
    "\n",
    "If the AdaBoost model is overfitting the data with a large number of boosted trees, then the learning rate hyper-parameter needs to be tuned (lowered). In lowering the learning rate, less importance will be placed on the previous batch of boosted trees, and the model will be able to better generalize on new data. In general, decreasing learning rate prevents overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdb32e9",
   "metadata": {},
   "source": [
    "### 5. Which of the following algorithm is NOT an ensemble learning algorithm?\n",
    "\n",
    "(a) support vector machine is not an ensemble learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6860b7",
   "metadata": {},
   "source": [
    "### 6. How does XGBoost with random forest as base learner work? Be specific.\n",
    "\n",
    "Random Forest is not an option for the 'booster' hyper-parameter in XGBoost, but it can be implemented by setting the 'num_parallel_trees' hyper-parameter to an integer greater than 1. Essentially what this does is it makes the model no longer consider only one tree for each boosting round (default 'num_parallel_trees' value = 1), but rather a number of parallel trees (the same principle as a Random Forest model). With each boosting round consisting of say, 500, num_parallel_trees, it has the same effect as using a Random Forest with 500 trees as the XGBoost base learner. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e0ab93",
   "metadata": {},
   "source": [
    "### 7. How do we select the best hyper-parameter combination for a given model?\n",
    "\n",
    "(b) Performance on the validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19fa8cf",
   "metadata": {},
   "source": [
    "### 8. The below chart shows the performance of a random forest models (with different number of trees). Using the above chart, answer parts (a)-(b). \n",
    "\n",
    "### (a) If a data scientist wants to tune the number of trees based on the F1-score, how many trees would he select? Be specific. \n",
    "To tune the number of trees based on the F1-score, the data scientist would select 400 trees because there is little to no prediction improvement in the training and testing sets after increasing the number of trees past 400. Anything more would result in an increased computation time with no model improvement.\n",
    "\n",
    "### (b) What is the reason there is big difference in the F1-score performance between the Train and Test datasets? Be specific.\n",
    "The main reason why there would be a huge difference in the F1-score performance between the train and test datasets is that the model is overfitting the data. Typically with hyper-parameter tuning, we create a model that fits the training data very well, but does not do a great job of generalizing on the test set. This results in a huge performance loss on the testing set. Another reason could be that the test data set is not representative of the training set which would also decrease test performance, but this is much less likely to be the root error compared to training an over-fit model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7afa639",
   "metadata": {},
   "source": [
    "### 9.  What is the difference between homogeneous and heterogeneous ensembles? Be specific.\n",
    "\n",
    "The difference between homogeneous and heterogeneous ensembles is the variety of modeling algorithms that are used to ensemble the predictions. In homegeneous ensembles, the same algorithm is used for all base learners on different distributions of the data set (i.e. using different subsets of the data to build a new learner). An example of this would be combining the predictions of three different Random Forest models built on three distinct subsets of the data. On the other hand, heterogeneous ensembles are created using multiple different algorithms as base learners. An example of this is combining the predictions of a Random Forest model, Support Vector Machine model, and XGBoost model on the same data set. Homogeneous uses only one algorithm, heterogeneous considers multiple. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc34fe5",
   "metadata": {},
   "source": [
    "### 10 Random forest is an example of\n",
    "(d) (a) Homogeneous ensemble and (c) Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b4f4be",
   "metadata": {},
   "source": [
    "## 11. A junior data scientist is working on building a classification framework that can help a small local credit union to identify fraudulent transactions. The data scientist collects a very small dataset with 100 observations, from which 4 observations are labelled as fraudulent, to start building and tuning the model. As part of the tuning process, the data scientist considers using the GridSearch function from scikit-learn with cv = 5. Explain why this approach is a good idea or not. Be specific.\n",
    "\n",
    "This sounds like a very poor idea. The GridSearchCV framework uses a stratified k-fold approach, which means that by setting cv = 5, the framework splits the complete 100 observation set into five data sets that are meant to contained a stratified sample of the class of interest (fraud transaction in this case). However, since there are not even five positive fraud cases, the GridSearchCV function would be (a) unable to split the data for the purpose of cross validation, or (b) able to split the data but one of the resulting data sets would have no positive fraud cases. Whichever one of these options is the case, it is a bad idea all around to use the GridSearch function in this scenario. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a2d36d",
   "metadata": {},
   "source": [
    "### 12. The typical hyper-parameters in a XGBoost model that data science practitioners tune are: n estimators, learning rate, max depth, min child weight, subsample, and colsample bytree.\n",
    "\n",
    "### (a) What is/are the hyper-parameters that may cause overfitting if we increase their values while holding the values of the hyper-parameters constant? Be specific.\n",
    "Of the considered hyper-parameters, if we increase the values for learning rate, max depth, subsample, and colsample bytree while holding all other values constant, we may see overfitting of the model. Increasing these specific hyper-parameters increases the complexity of the model and increases the model's reliance on the training data observations. This is a recipe for overfitting. \n",
    "\n",
    "\n",
    "### (b) What is/are the hyper-parameters that prevents overfitting if we increase their values while holding the values of the hyper-parameters constant? Be specific.\n",
    "Of the considered hyper-parameters, if we increase the value for min child weight while holding all other values constant, we may see prevention against overfitting of the model. By increasing this hyper-parameter, the number of splits in the data is limited and therefore prevents against overfitting of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dadfc8",
   "metadata": {},
   "source": [
    "### 13. In tree-based models, how does a data scientist optimize the number of trees? Be specific.\n",
    "\n",
    "For tree-based models, a data scientist can optimize the number of trees by tuning it as a hyper-parameter. For tree-based models such as AdaBoost or Random Forest, n_estimators represents the number of trees to be considered and this could be tuned within the GridSearch framework. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa3f377",
   "metadata": {},
   "source": [
    "### 14. A Machine Learning Specialist is assigned to a team that is responsible for tuning an XGBoost model to ensure that it performs correctly on test data. However, when dealing with unknown data, this does not operate as planned. The following table summarizes the existing hyper-parameters:\n",
    "\n",
    "XGBoost_params = {’n_estimators’: 2000, ’max_depth’: 30, ’min_child_weight’: 3, ’subsample’: 0.9, ’objective’: ’reg:squarederror’}\n",
    "\n",
    "### (a) What type of task the machine learning specialist is working on? Classification or regression? Be specific.\n",
    "The machine learning specialist is working on a regression task. \"reg:squarederror\" is the default setting for the objective hyper-parameter in in an XGBoost regression task. \n",
    "\n",
    "### (b) Given the existing XGBoost hyper-parameters, what may be the reason that the XGBoost model is not performing as expected? Be specific.\n",
    "Given the existing hyper-parameters for the XGBoost model, it may not be performing as expected because the n_estimators and max_depth values are promoting a model that is overfit. Using 2000 trees with a max depth of 30 is just a recipe for overfitting the training data. Although increasing the number of trees may improve the result, 2000 may be too many to consider. Similarly, decreasing the max depth will prevent the likelihood that the model is overfit. Overall, the \"Specialist\" should consider changes to n_estimators and max_depth to see some increased model performance and better generalization on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19419043",
   "metadata": {},
   "source": [
    "### 15. Considering the train.csv and test.csv data files containing information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. The goal is to predict default payment next month on the test.csv data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15b1b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42cd2ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing necessary libraries\n",
    "import boto3\n",
    "import pandas as pd; pd.set_option('display.max_columns', 50)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import precision_recall_cutoff_exam2 as prc\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d59bc0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>PAY_6</th>\n",
       "      <th>BILL_AMT1</th>\n",
       "      <th>BILL_AMT2</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>400000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55773</td>\n",
       "      <td>55917</td>\n",
       "      <td>51389</td>\n",
       "      <td>48272</td>\n",
       "      <td>49478</td>\n",
       "      <td>51242</td>\n",
       "      <td>3028</td>\n",
       "      <td>3023</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>38662</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>140</td>\n",
       "      <td>3230</td>\n",
       "      <td>3011</td>\n",
       "      <td>1964</td>\n",
       "      <td>1883</td>\n",
       "      <td>1538</td>\n",
       "      <td>3230</td>\n",
       "      <td>3011</td>\n",
       "      <td>1964</td>\n",
       "      <td>1883</td>\n",
       "      <td>1538</td>\n",
       "      <td>1911</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>270000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59710</td>\n",
       "      <td>49986</td>\n",
       "      <td>104390</td>\n",
       "      <td>94856</td>\n",
       "      <td>86461</td>\n",
       "      <td>83650</td>\n",
       "      <td>1808</td>\n",
       "      <td>69563</td>\n",
       "      <td>2891</td>\n",
       "      <td>2689</td>\n",
       "      <td>3012</td>\n",
       "      <td>2771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>280000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>280913</td>\n",
       "      <td>283222</td>\n",
       "      <td>273160</td>\n",
       "      <td>257689</td>\n",
       "      <td>193231</td>\n",
       "      <td>191143</td>\n",
       "      <td>11052</td>\n",
       "      <td>9563</td>\n",
       "      <td>15017</td>\n",
       "      <td>5374</td>\n",
       "      <td>5420</td>\n",
       "      <td>6021</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>1512</td>\n",
       "      <td>2458</td>\n",
       "      <td>664</td>\n",
       "      <td>1814</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>664</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0     400000    1          1         2   32      0      0      0      0   \n",
       "1     120000    2          2         2   30     -1     -1     -1     -1   \n",
       "2     270000    2          2         2   32      0      0      0      0   \n",
       "3     280000    2          2         1   27      0      0      0      0   \n",
       "4      30000    2          1         2   27      0      0     -1      0   \n",
       "\n",
       "   PAY_5  PAY_6  BILL_AMT1  BILL_AMT2  BILL_AMT3  BILL_AMT4  BILL_AMT5  \\\n",
       "0      0      0      55773      55917      51389      48272      49478   \n",
       "1     -1     -1        140       3230       3011       1964       1883   \n",
       "2      0      0      59710      49986     104390      94856      86461   \n",
       "3      0      0     280913     283222     273160     257689     193231   \n",
       "4      0     -2       1512       2458        664       1814          0   \n",
       "\n",
       "   BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \\\n",
       "0      51242      3028      3023      3000      3000      3000     38662   \n",
       "1       1538      3230      3011      1964      1883      1538      1911   \n",
       "2      83650      1808     69563      2891      2689      3012      2771   \n",
       "3     191143     11052      9563     15017      5374      5420      6021   \n",
       "4          0      1000       664      1500         0         0         0   \n",
       "\n",
       "   default payment next month  \n",
       "0                           0  \n",
       "1                           0  \n",
       "2                           0  \n",
       "3                           0  \n",
       "4                           0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## a) Using the pandas library to read the train.csv and test.csv data files and create two data-frames called train and test\n",
    "\n",
    "## Defining the bucket\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'data-448-bucket-callaghan'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "file_key = 'train(1).csv'\n",
    "file_key2 = 'test(1).csv'\n",
    "\n",
    "bucket_object = bucket.Object(file_key)\n",
    "bucket_object2 = bucket.Object(file_key2)\n",
    "\n",
    "file_object = bucket_object.get()\n",
    "file_object2 = bucket_object2.get()\n",
    "\n",
    "file_content_stream = file_object.get('Body')\n",
    "file_content_stream2 = file_object2.get('Body')\n",
    "\n",
    "train = pd.read_csv(file_content_stream)\n",
    "test = pd.read_csv(file_content_stream2)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "476f016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Engineering features from Exam 1\n",
    "\n",
    "## Train set:\n",
    "\n",
    "## Most common repayment status\n",
    "train['Most_Common'] = np.nan\n",
    "for i in range(0, train.shape[0]):\n",
    "    train.at[i, 'Most_Common'] = train[['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']].loc[i].mode()[0]\n",
    "\n",
    "## From plot tree:\n",
    "train['Tree2'] = np.where((train['PAY_0'] <= 1.5) & (train['PAY_2'] <= 1.5) & (train['PAY_AMT3'] > 395.0), 1, 0)\n",
    "train['Tree6'] = np.where((train['PAY_0'] > 1.5) & (train['PAY_6'] <= 1.0) & (train['BILL_AMT1'] > 649.5), 1, 0)\n",
    "train['Tree7'] = np.where((train['PAY_0'] > 1.5) & (train['PAY_6'] > 1.0) & (train['PAY_AMT3'] <= 14177.0), 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "## Test set:\n",
    "\n",
    "## Most common repayment status\n",
    "test['Most_Common'] = np.nan\n",
    "for i in range(0, test.shape[0]):\n",
    "    test.at[i, 'Most_Common'] = test[['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']].loc[i].mode()[0]\n",
    "\n",
    "## From plot tree:\n",
    "test['Tree2'] = np.where((test['PAY_0'] <= 1.5) & (test['PAY_2'] <= 1.5) & (test['PAY_AMT3'] > 395.0), 1, 0)\n",
    "test['Tree6'] = np.where((test['PAY_0'] > 1.5) & (test['PAY_6'] <= 1.0) & (test['BILL_AMT1'] > 649.5), 1, 0)\n",
    "test['Tree7'] = np.where((test['PAY_0'] > 1.5) & (test['PAY_6'] > 1.0) & (test['PAY_AMT3'] <= 14177.0), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35afab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## b) Splitting the train data-frame intro training (80%) and validation (20%) (taking into account the proportions of 0s and 1s)\n",
    "\n",
    "## Defining the input and target variables\n",
    "X = train.drop(columns = ['default payment next month'])\n",
    "Y = train['default payment next month']\n",
    "\n",
    "## Splitting the data\n",
    "X_training, X_validation, Y_training, Y_validation = train_test_split(X, Y, test_size = 0.2, stratify = Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e80728",
   "metadata": {},
   "source": [
    "#### Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e7ecee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Cutoff of Random Forest Model: 0.156\n",
      "\n",
      "F1-Score of Random Forest Model: 51.67 %\n"
     ]
    }
   ],
   "source": [
    "## c) Using the top 7 variables from Exercise 15 part (e) in Exam 1 to  build a model on the training data-frame\n",
    "\n",
    "## Redefining the input and target variables\n",
    "X_training = X_training[['PAY_0', 'PAY_2', 'Most_Common', 'Tree2', 'Tree6', 'PAY_3', 'Tree7']]\n",
    "X_validation = X_validation[['PAY_0', 'PAY_2', 'Most_Common', 'Tree2', 'Tree6', 'PAY_3', 'Tree7']]\n",
    "test = test[['PAY_0', 'PAY_2', 'Most_Common', 'Tree2', 'Tree6', 'PAY_3', 'Tree7']]\n",
    "\n",
    "## Building a Random Forest Model with default hyper-parameters\n",
    "rf_md = RandomForestClassifier().fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the validation set\n",
    "rf_preds = rf_md.predict_proba(X_validation)[:, 1]\n",
    "\n",
    "## Extracting estimated labels using the PRC function\n",
    "rf_labels = prc.precision_recall_cutoff(Y_validation, rf_preds)\n",
    "\n",
    "## Extracting optimal cutoff using the PRC function\n",
    "rf_cutoff = prc.precision_recall_cutoff_cutoff(Y_validation, rf_preds)\n",
    "\n",
    "## Reporting the optimal cutoff value\n",
    "print('Optimal Cutoff of Random Forest Model:', round(rf_cutoff, 3))\n",
    "\n",
    "## Reporting the F1-Score of the model\n",
    "print('\\nF1-Score of Random Forest Model:', round(f1_score(Y_validation, rf_labels) * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ed3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tuning the Random Forest model on the validation data-frame\n",
    "\n",
    "## Defining the parameter dictionary\n",
    "rf_param_grid = {'n_estimators': [100, 300, 500], 'max_depth': [3, 5, 7], 'min_samples_split': [5, 10, 15], \n",
    "                  'min_samples_leaf': [5, 10, 15]}\n",
    "\n",
    "## Running GridSearchCV with 3 folds\n",
    "rf_grid_search = GridSearchCV(RandomForestClassifier(), rf_param_grid, cv = 3, scoring = 'f1', n_jobs = -1).fit(X_validation, Y_validation)\n",
    "\n",
    "## Extracting the best hyper-parameters\n",
    "print('Optimal hyper-parameters for Random Forest Model: \\n', rf_grid_search.best_params_)\n",
    "print('\\nOptimal F1-Score:\\n', round(rf_grid_search.best_score_ * 100, 2), '%')\n",
    "\n",
    "\n",
    "## Ran in Python Script (Exam_2_Script.py) ##\n",
    "\n",
    "'''\n",
    "Optimal results:\n",
    "\n",
    "max_depth = 5\n",
    "min_samples_leaf = 15\n",
    "min_samples_split = 10\n",
    "n_estimators = 300\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a7d3a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Cutoff of Random Forest Model: 0.208\n",
      "\n",
      "F1-Score of Random Forest Model: 54.39 %\n"
     ]
    }
   ],
   "source": [
    "## Building a Random Forest model with the optimal hyper-parameters\n",
    "rf_md = RandomForestClassifier(max_depth = 5, min_samples_leaf = 15, min_samples_split = 10, \n",
    "                               n_estimators = 300).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the validation set\n",
    "rf_preds = rf_md.predict_proba(X_validation)[:, 1]\n",
    "\n",
    "## Extracting estimated labels using the PRC function\n",
    "rf_labels = prc.precision_recall_cutoff(Y_validation, rf_preds)\n",
    "\n",
    "## Extracting optimal cutoff using the PRC function\n",
    "rf_cutoff = prc.precision_recall_cutoff_cutoff(Y_validation, rf_preds)\n",
    "\n",
    "## Reporting the optimal cutoff value\n",
    "print('Optimal Cutoff of Random Forest Model:', round(rf_cutoff, 3))\n",
    "\n",
    "## Reporting the F1-Score of the model\n",
    "print('\\nF1-Score of Random Forest Model:', round(f1_score(Y_validation, rf_labels) * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc1be2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally, using the optimal model to predict the likelihood of default payment next month on the test\n",
    "\n",
    "rf_test_preds = rf_md.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd32cf",
   "metadata": {},
   "source": [
    "#### AdaBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d37f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Cutoff of AdaBoost Model: 0.493\n",
      "\n",
      "F1-Score of AdaBoost Model: 53.41 %\n"
     ]
    }
   ],
   "source": [
    "## d) Using the top 7 variables from Exercise 15 part (e) in Exam 1 to  build a model on the training data-frame\n",
    "\n",
    "## Building an AdaBoost Model with default hyper-parameters\n",
    "ada_md = AdaBoostClassifier().fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the validation set\n",
    "ada_preds = ada_md.predict_proba(X_validation)[:, 1]\n",
    "\n",
    "## Extracting estimated labels using the PRC function\n",
    "ada_labels = prc.precision_recall_cutoff(Y_validation, ada_preds)\n",
    "\n",
    "## Extracting optimal cutoff using the PRC function\n",
    "ada_cutoff = prc.precision_recall_cutoff_cutoff(Y_validation, ada_preds)\n",
    "\n",
    "## Reporting the optimal cutoff value\n",
    "print('Optimal Cutoff of AdaBoost Model:', round(ada_cutoff, 3))\n",
    "\n",
    "## Reporting the F1-Score of the model\n",
    "print('\\nF1-Score of AdaBoost Model:', round(f1_score(Y_validation, ada_labels) * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e07d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tuning the AdaBoost model on the validation data-frame\n",
    "\n",
    "## Defining the parameter dictionary\n",
    "ada_param_grid = {'n_estimators': [100, 300, 500], 'base_estimator__min_samples_split': [10, 15], \n",
    "                  'base_estimator__min_samples_leaf': [10, 15], 'base_estimator__max_depth': [3, 5, 7], \n",
    "                  'learning_rate': [0.001, 0.01, 0.1]}\n",
    "\n",
    "## Running GridSearchCV with 3 folds\n",
    "ada_grid_search = GridSearchCV(AdaBoostClassifier(base_estimator = DecisionTreeClassifier()), ada_param_grid, \n",
    "                               cv = 3, scoring = 'f1', n_jobs = -1).fit(X_validation, Y_validation)\n",
    "\n",
    "## Extracting the best hyper-parameters\n",
    "print('Optimal hyper-parameters for AdaBoost Model: \\n', ada_grid_search.best_params_)\n",
    "print('\\nOptimal F1-Score:\\n', round(ada_grid_search.best_score_ * 100, 2), '%')\n",
    "\n",
    "\n",
    "## Ran in Python Script (Exam_2_Script.py) ##\n",
    "\n",
    "'''\n",
    "Optimal results:\n",
    "\n",
    "max_depth = 7\n",
    "min_samples_leaf = 10\n",
    "min_samples_split = 10\n",
    "n_estimators = 300\n",
    "learning_rate = 0.001\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "295a21af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Cutoff of AdaBoost Model: 0.272\n",
      "\n",
      "F1-Score of AdaBoost Model: 54.18 %\n"
     ]
    }
   ],
   "source": [
    "## Building a AdaBoost model with the optimal hyper-parameters\n",
    "ada_md = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(min_samples_split = 10, min_samples_leaf = 10, max_depth = 7), \n",
    "                            n_estimators = 300, learning_rate = 0.001).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the validation set\n",
    "ada_preds = ada_md.predict_proba(X_validation)[:, 1]\n",
    "\n",
    "## Extracting estimated labels using the PRC function\n",
    "ada_labels = prc.precision_recall_cutoff(Y_validation, ada_preds)\n",
    "\n",
    "## Extracting optimal cutoff using the PRC function\n",
    "ada_cutoff = prc.precision_recall_cutoff_cutoff(Y_validation, ada_preds)\n",
    "\n",
    "## Reporting the optimal cutoff value\n",
    "print('Optimal Cutoff of AdaBoost Model:', round(ada_cutoff, 3))\n",
    "\n",
    "## Reporting the F1-Score of the model\n",
    "print('\\nF1-Score of AdaBoost Model:', round(f1_score(Y_validation, ada_labels) * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0333b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally, using the optimal model to predict the likelihood of default payment next month on the test\n",
    "\n",
    "ada_test_preds = ada_md.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc90d4",
   "metadata": {},
   "source": [
    "#### XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aebaba6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Cutoff of XGBoost Model: 0.241\n",
      "\n",
      "F1-Score of XGBoost Model: 53.7 %\n"
     ]
    }
   ],
   "source": [
    "## e) Using the top 7 variables from Exercise 15 part (e) in Exam 1 to  build a model on the training data-frame\n",
    "\n",
    "## Building XGBoost Model with default hyper-parameters\n",
    "xgb_md = XGBClassifier(eval_metric = 'error', use_label_encoder = False).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the validation set\n",
    "xgb_preds = xgb_md.predict_proba(X_validation)[:, 1]\n",
    "\n",
    "## Extracting estimated labels using the PRC function\n",
    "xgb_labels = prc.precision_recall_cutoff(Y_validation, xgb_preds)\n",
    "\n",
    "## Extracting optimal cutoff using the PRC function\n",
    "xgb_cutoff = prc.precision_recall_cutoff_cutoff(Y_validation, xgb_preds)\n",
    "\n",
    "## Reporting the optimal cutoff value\n",
    "print('Optimal Cutoff of XGBoost Model:', round(xgb_cutoff, 3))\n",
    "\n",
    "## Reporting the F1-Score of the model\n",
    "print('\\nF1-Score of XGBoost Model:', round(f1_score(Y_validation, xgb_labels) * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed1473",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tuning the XGBoost model on the validation data-frame\n",
    "\n",
    "## Defining the parameter dictionary\n",
    "xgb_param_grid = {'n_estimators': [300, 500], 'max_depth': [5, 7], 'min_child_weight': [5, 7], \n",
    "                  'learning_rate' : [0.01, 0.001], 'gamma': [0.1, 0.01], 'subsample': [0.8, 1], \n",
    "                  'colsample_bytree': [0.8, 1], 'early_stopping_rounds': [100]}\n",
    "\n",
    "## Running GridSearchCV with 3 folds\n",
    "xgb_grid_search = GridSearchCV(XGBClassifier(eval_metric = 'error', use_label_encoder = False), xgb_param_grid, cv = 3, scoring = 'f1', \n",
    "                               n_jobs = -1).fit(X_validation, Y_validation)\n",
    "\n",
    "## Extracting the best hyper-parameters\n",
    "print('Optimal hyper-parameters for XGBoost Model: \\n', xgb_grid_search.best_params_)\n",
    "print('\\nOptimal F1-Score:\\n', round(xgb_grid_search.best_score_ * 100, 2), '%')\n",
    "\n",
    "\n",
    "## Ran in Python Script (Exam_2_Script.py) ##\n",
    "\n",
    "\n",
    "'''\n",
    "Optimal results:\n",
    "\n",
    "max_depth = 7\n",
    "n_estimators = 500\n",
    "learning_rate = 0.01\n",
    "colsample_bytree = 1\n",
    "early_stopping_rounds = 100\n",
    "gamma = 0.1\n",
    "min_child_weight = 5\n",
    "subsample = 1\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcdd9d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:04:12] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"early_stopping_rounds\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "Optimal Cutoff of XGBoost Model: 0.243\n",
      "\n",
      "F1-Score of XGBoost Model: 54.12 %\n"
     ]
    }
   ],
   "source": [
    "## Building a XGBoost model with the optimal hyper-parameters\n",
    "xgb_md = XGBClassifier(n_estimators = 500, max_depth = 7, min_child_weight = 5, learning_rate = 0.01,\n",
    "                      gamma = 0.1, subsample = 1, colsample_bytree = 1, early_stopping_rounds = 100, \n",
    "                      eval_metric = 'error', use_label_encoder = False).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the validation set\n",
    "xgb_preds = xgb_md.predict_proba(X_validation)[:, 1]\n",
    "\n",
    "## Extracting estimated labels using the PRC function\n",
    "xgb_labels = prc.precision_recall_cutoff(Y_validation, xgb_preds)\n",
    "\n",
    "## Extracting optimal cutoff using the PRC function\n",
    "xgb_cutoff = prc.precision_recall_cutoff_cutoff(Y_validation, xgb_preds)\n",
    "\n",
    "## Reporting the optimal cutoff value\n",
    "print('Optimal Cutoff of XGBoost Model:', round(xgb_cutoff, 3))\n",
    "\n",
    "## Reporting the F1-Score of the model\n",
    "print('\\nF1-Score of XGBoost Model:', round(f1_score(Y_validation, xgb_labels) * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55bf5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally, using the optimal model to predict the likelihood of default payment next month on the test\n",
    "\n",
    "xgb_test_preds = xgb_md.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bab474",
   "metadata": {},
   "source": [
    "#### Meta-Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9ad3f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal hyper-parameters for Meta-Learner Random Forest Model: \n",
      " {'max_depth': 7, 'min_samples_leaf': 5, 'min_samples_split': 15, 'n_estimators': 300}\n",
      "\n",
      "Optimal F1-Score:\n",
      " 46.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nOptimal results:\\n\\nmax_depth = 7\\nmin_samples_leaf = 5\\nmin_samples_split = 10\\nn_estimators = 500\\n    \\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## f) Using the predictions on the validation data-frame as inputs from parts (c)-(d)-(e) and the actual default payment \n",
    "## next month values from the validation data-frame to build a meta-learner to predict default payment next month\n",
    "\n",
    "## Building the meta-learner data frame\n",
    "meta_data = pd.DataFrame({'rf': rf_preds, 'ada': ada_preds, 'xgb': xgb_preds, 'Y_validation': Y_validation})\n",
    "\n",
    "## Building the meta-learner test data frame\n",
    "meta_data_test = pd.DataFrame({'rf': rf_test_preds, 'ada': ada_test_preds, 'xgb': xgb_test_preds})\n",
    "\n",
    "## Defining the input and target variables\n",
    "X_meta = meta_data[['rf', 'ada', 'xgb']]\n",
    "Y_meta = meta_data['Y_validation']\n",
    "\n",
    "## Defining the parameter dictionary\n",
    "meta_param_grid = {'n_estimators': [100, 300, 500], 'max_depth': [3, 5, 7], 'min_samples_split': [5, 10, 15], \n",
    "                  'min_samples_leaf': [5, 10, 15]}\n",
    "\n",
    "## Running GridSearchCV with 3 folds\n",
    "meta_grid_search = GridSearchCV(RandomForestClassifier(), meta_param_grid, cv = 3, scoring = 'f1', n_jobs = -1).fit(X_meta, Y_meta)\n",
    "\n",
    "## Extracting the best hyper-parameters\n",
    "print('Optimal hyper-parameters for Meta-Learner Random Forest Model: \\n', meta_grid_search.best_params_)\n",
    "print('\\nOptimal F1-Score:\\n', round(meta_grid_search.best_score_ * 100, 2), '%')\n",
    "\n",
    "'''\n",
    "Optimal results:\n",
    "\n",
    "max_depth = 7\n",
    "min_samples_leaf = 5\n",
    "min_samples_split = 15\n",
    "n_estimators = 300\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ea0afcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Cutoff of Meta-Learner Model: 0.183\n",
      "\n",
      "F1-Score of Meta-Learner Model: 53.45 %\n"
     ]
    }
   ],
   "source": [
    "## Splitting the data\n",
    "X_meta_training, X_meta_validation, Y_meta_training, Y_meta_validation = train_test_split(X_meta, Y_meta, \n",
    "                                                                                          test_size = 0.2, stratify = Y_meta)\n",
    "\n",
    "## Building a Meta-Learner model with the optimal hyper-parameters\n",
    "meta_md = RandomForestClassifier(n_estimators = 300, max_depth = 7, min_samples_split = 15, \n",
    "                                 min_samples_leaf = 5).fit(X_meta_training, Y_meta_training)\n",
    "\n",
    "## Predicting on the validation set\n",
    "meta_preds = meta_md.predict_proba(X_meta_validation)[:, 1]\n",
    "\n",
    "## Extracting estimated labels using the PRC function\n",
    "meta_labels = prc.precision_recall_cutoff(Y_meta_validation, meta_preds)\n",
    "\n",
    "## Extracting optimal cutoff using the PRC function\n",
    "meta_cutoff = prc.precision_recall_cutoff_cutoff(Y_meta_validation, meta_preds)\n",
    "\n",
    "## Reporting the optimal cutoff value\n",
    "print('Optimal Cutoff of Meta-Learner Model:', round(meta_cutoff, 3))\n",
    "\n",
    "## Reporting the F1-Score of the model\n",
    "print('\\nF1-Score of Meta-Learner Model:', round(f1_score(Y_meta_validation, meta_labels) * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27669695",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally, using the optimal meta-learner to predict the likelihood of default payment next month on the test\n",
    "meta_test_preds = meta_md.predict_proba(meta_data_test)[:, 1]\n",
    "\n",
    "## Creating a data-frame to store results\n",
    "meta_likelihoods = pd.DataFrame({'Likelihoods': meta_test_preds})\n",
    "\n",
    "## Exporting predicted likelihoods as a csv file\n",
    "meta_likelihoods.to_csv('meta_likelihoods.csv', index = False)\n",
    "\n",
    "## Associated cutoff value = 0.183"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
